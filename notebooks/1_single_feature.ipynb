{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A real-life use case: fraud detection\n",
        "\n",
        "Imagine you’re a data scientist who works at a large bank. You have been tasked\n",
        "with one of the most challenging problems in banking today: identifying\n",
        "fraudulent transactions. The bank receives transaction details from its credit\n",
        "card customers in a Kafka topic, which include information about the transaction\n",
        "date and time, transaction amount, transaction location, merchant, category of\n",
        "purchase, and so on. Given the nature of the data, you want to use Apache Flink\n",
        "for its stream processing capabilities and to develop machine learning features\n",
        "that can be used to identify fraud.\n",
        "\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "* Docker Compose: This tutorial uses Docker Compose to manage an Apache Kafka\n",
        "environment (including sample data generation) and a Flink cluster (for remote\n",
        "execution). You can [download and install Docker Compose from the official\n",
        "website](https://docs.docker.com/compose/install/).\n",
        "* JDK 11 release: Flink requires Java 11.\n",
        "* Python 3.9 or 3.10.\n",
        "* Follow [the setup tutorial](0_setup.qmd) to install the Flink backend for Ibis.\n",
        "* Clone the [example repository](https://github.com/ibis-project/ibis-flink-tutorial).\n",
        "\n",
        "\n",
        "### Spinning up the services using Docker Compose\n",
        "\n",
        "From your project directory, run `docker compose up -d` to create Kafka topics,\n",
        "generate sample data, and launch a Flink cluster in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | code-fold: true\n",
        "# | include: false\n",
        "!pip install apache-flink\n",
        "!git clone https://github.com/ibis-project/ibis-flink-tutorial\n",
        "!cd ibis-flink-tutorial && docker compose up -d && sleep 10 && cd ..\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "consumer = KafkaConsumer(\"transaction\", auto_offset_reset=\"earliest\")\n",
        "for _, msg in zip(range(10), consumer):\n",
        "    # this ensures that messages exist in the `transaction` topic before\n",
        "    # proceeding\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip}\n",
        "Running `docker compose up` with the `-d` flag runs it in\n",
        "detached mode, where containers are run in the background. While this frees up\n",
        "the terminal for you to run other commands, it also hides error messages.\n",
        "\n",
        "Depending on whether the container images are already available locally, setting\n",
        "up the containers may take anywhere from 10 seconds to a minute. If it's your\n",
        "first time running this command, it's best to run it in the foreground so that\n",
        "you can monitor the progress of setup.\n",
        ":::\n",
        "\n",
        "This should set up a `transaction` topic in the Kafka cluster that contains\n",
        "messages that look like the following:\n",
        "\n",
        "```\n",
        "{ \"trans_date_trans_time\": \"2012-02-23 00:10:01\", \"cc_num\":\n",
        "4428780000000000000, \"merchant\": \"fraud_Olson, Becker and Koch\", \"category\":\n",
        "\"gas_transport\", \"amt\": 82.55, \"first\": \"Richard\", \"last\": \"Waters\", \"zipcode\":\n",
        "\"53186\", \"dob\": \"1/2/46\", \"trans_num\": \"dbf31d83eebdfe96d2fa213df2043586\",\n",
        "\"is_fraud\": 0, \"user_id\": 7109464218691269943 }\n",
        "```\n",
        "\n",
        "::: {.callout-warning}\n",
        "Do not proceed to the next section until messages are\n",
        "flowing into the `transaction` topic!\n",
        ":::\n",
        "\n",
        "\n",
        "### Connect to a Flink environment session\n",
        "\n",
        "We can connect to a Flink environment session by creating a\n",
        "`pyflink.table.TableEnvironment` and passing this to Flink backend’s `connect`\n",
        "method. For this tutorial, we are going to use Flink in streaming mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
        "\n",
        "import ibis\n",
        "\n",
        "env_settings = EnvironmentSettings.in_streaming_mode()\n",
        "table_env = TableEnvironment.create(env_settings)\n",
        "table_env.get_config().set(\"parallelism.default\", \"1\")\n",
        "connection = ibis.flink.connect(table_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Kafka connector isn’t part of the binary distribution. In order to connect\n",
        "to a Kafka source/sink, we need to [download the JAR\n",
        "file](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.0.2-1.18/flink-sql-connector-kafka-3.0.2-1.18.jar)\n",
        "and manually add it into the classpath:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# | code-fold: true\n",
        "# | include: false\n",
        "!wget -N https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.0.2-1.18/flink-sql-connector-kafka-3.0.2-1.18.jar\n",
        "connection.raw_sql(\"ADD JAR './flink-sql-connector-kafka-3.0.2-1.18.jar'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we’ve set up the Flink table environment, we’re ready to connect to\n",
        "data!\n",
        "\n",
        "\n",
        "### Connect to a data source\n",
        "\n",
        "In order to experiment with the data in the Kafka topic and create\n",
        "transformations on top of it, we need to first define and connect to the data\n",
        "source.\n",
        "\n",
        "While we’re dealing with a continuous stream of data here, Flink and Ibis\n",
        "abstract differences in the underlying implementation between tables and\n",
        "streams, so that, conceptually, we can simply treat our Kafka topic as a table.\n",
        "\n",
        "To connect to our `transaction` Kafka topic, we need to provide a table name,\n",
        "schema of the data, and [connector\n",
        "configurations](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/kafka/#connector-options).\n",
        "The schema of the data must contain a subset of the fields in the actual Kafka\n",
        "topic. Because this is a streaming job, we also want to define a watermark\n",
        "strategy for the data source by specifying the timestamp column (`time_col`) and\n",
        "a time duration during which late events are accepted (`allowed_delay`). (If you\n",
        "are not already familiar with these concepts, you can check out [Flink’s\n",
        "documentation](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/operators/windows/)\n",
        "for more details.) Note that [Flink requires the timestamp column to be of data\n",
        "type\n",
        "TIMESTAMP(3)](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#watermark)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ibis\n",
        "import ibis.expr.datatypes as dt\n",
        "import ibis.expr.schema as sch\n",
        "\n",
        "source_schema = sch.Schema(\n",
        "    {\n",
        "        \"user_id\": dt.int64,\n",
        "        \"trans_date_trans_time\": dt.timestamp(scale=3),\n",
        "        \"cc_num\": dt.int64,\n",
        "        \"amt\": dt.float64,\n",
        "        \"trans_num\": dt.str,\n",
        "        \"merchant\": dt.str,\n",
        "        \"category\": dt.str,\n",
        "        \"is_fraud\": dt.int32,\n",
        "        \"first\": dt.str,\n",
        "        \"last\": dt.str,\n",
        "        \"dob\": dt.str,\n",
        "        \"zipcode\": dt.str,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Configure the source table with Kafka connector properties.\n",
        "source_configs = {\n",
        "    \"connector\": \"kafka\",\n",
        "    \"topic\": \"transaction\",\n",
        "    \"properties.bootstrap.servers\": \"localhost:9092\",\n",
        "    \"properties.group.id\": \"consumer_group_0\",\n",
        "    \"scan.startup.mode\": \"earliest-offset\",\n",
        "    \"format\": \"json\",\n",
        "}\n",
        "\n",
        "# Create the source table using the defined schema, Kafka connector properties,\n",
        "# and set watermarking for real-time processing with a 15-second allowed\n",
        "# lateness.\n",
        "source_table = connection.create_table(\n",
        "    \"transaction\",\n",
        "    schema=source_schema,\n",
        "    tbl_properties=source_configs,\n",
        "    watermark=ibis.watermark(\n",
        "        time_col=\"trans_date_trans_time\", allowed_delay=ibis.interval(seconds=15)\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’re ready to write some transformations!\n",
        "\n",
        "\n",
        "### Create transformations\n",
        "\n",
        "Which signs could be indicative of suspected fraud in a credit card? Oftentimes,\n",
        "we’re looking for abnormalities in user behaviors, for example, an excessively\n",
        "large transaction amount, unusually frequent transactions during a short period\n",
        "of time, etc. Based on this, the average transaction amount and the total\n",
        "transaction count over the past five hours may be useful features. Let’s write\n",
        "out each of these using Ibis API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_trans_amt_last_360m_agg = source_table[\n",
        "    source_table.user_id,\n",
        "    # Calculate the average transaction amount over the past six hours\n",
        "    source_table.amt.mean()\n",
        "    .over(\n",
        "        ibis.window(\n",
        "            group_by=source_table.user_id,\n",
        "            order_by=source_table.trans_date_trans_time,\n",
        "            range=(-ibis.interval(minutes=360), 0),\n",
        "        )\n",
        "    )\n",
        "    .name(\"user_mean_trans_amt_last_360min\"),\n",
        "    # Calculate the total transaction count over the past six hours\n",
        "    source_table.amt.count()\n",
        "    .over(\n",
        "        ibis.window(\n",
        "            group_by=source_table.user_id,\n",
        "            order_by=source_table.trans_date_trans_time,\n",
        "            range=(-ibis.interval(minutes=360), 0),\n",
        "        )\n",
        "    )\n",
        "    .name(\"user_trans_count_last_360min\"),\n",
        "    source_table.trans_date_trans_time,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`over()` creates an [over\n",
        "aggregation](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/over-agg/)\n",
        "in Flink, which computes an aggregated value for every input row. More\n",
        "specifically, this means that an aggregation result is computed and emitted for\n",
        "every new record flowing into the upstream Kafka topic.\n",
        "\n",
        "The issue with over aggregation is that, if there is no new transaction for a\n",
        "specific user during a time window, there would be no aggregation result written\n",
        "to the sink. In other words, the user would never show up in the result table if\n",
        "they never made a transaction.\n",
        "\n",
        "Alternatively, we can compute aggregations using [Flink’s windowing table-valued\n",
        "functions](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/window-tvf/).\n",
        "This allows more flexibility in defining windows and when results are computed\n",
        "and emitted into the sink. There are three types of windowing TVFs available in\n",
        "Flink: tumble, hop, and cumulate. Let’s define the same features with tumble\n",
        "windows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "windowed_stream = source_table.window_by(\n",
        "    time_col=source_table.trans_date_trans_time,\n",
        ").tumble(window_size=ibis.interval(minutes=360))\n",
        "\n",
        "user_trans_amt_last_360m_agg_windowed_stream = windowed_stream.group_by(\n",
        "    [\"window_start\", \"window_end\", \"user_id\"]\n",
        ").agg(\n",
        "    user_mean_trans_amt_last_360min=windowed_stream.amt.mean(),\n",
        "    user_trans_count_last_360min=windowed_stream.amt.count(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to a data sink\n",
        "\n",
        "We’re creating streaming jobs to continuously process upstream data, which could\n",
        "be infinite. Therefore, we want to have the job continuously running and write\n",
        "results into a data sink. Here, we’re simply going to write results into a\n",
        "separate Kafka topic named `user_trans_amt_last_360min` for convenient\n",
        "downstream processing.\n",
        "\n",
        "We can define a data sink in virtually the same exact way in which we defined\n",
        "our data source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sink_schema = sch.Schema(\n",
        "    {\n",
        "        \"user_id\": dt.int64,\n",
        "        \"user_mean_trans_amt_last_360min\": dt.float64,\n",
        "        \"user_trans_count_last_360min\": dt.int64,\n",
        "        \"trans_date_trans_time\": dt.timestamp(scale=3),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Configure the sink table with Kafka connector properties for writing results.\n",
        "sink_configs = {\n",
        "    \"connector\": \"kafka\",\n",
        "    \"topic\": \"user_trans_amt_last_360min\",\n",
        "    \"properties.bootstrap.servers\": \"localhost:9092\",\n",
        "    \"format\": \"json\",\n",
        "}\n",
        "\n",
        "sink_table = connection.create_table(\n",
        "    \"user_trans_amt_last_360min\",\n",
        "    schema=sink_schema,\n",
        "    tbl_properties=sink_configs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step is to connect the pieces and actually write our query results into\n",
        "the sink table that we had just created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection.insert(\"user_trans_amt_last_360min\",\n",
        "user_trans_amt_last_360m_agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This step is exactly the same for windowing TVFs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sink_schema = sch.Schema(\n",
        "    {\n",
        "        \"window_start\": dt.timestamp(scale=3),\n",
        "        \"window_end\": dt.timestamp(scale=3),\n",
        "        \"user_id\": dt.int64,\n",
        "        \"user_mean_trans_amt_last_360min\": dt.float64,\n",
        "        \"user_trans_count_last_360min\": dt.int64,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Configure the sink table with Kafka connector properties for writing results.\n",
        "sink_configs = {\n",
        "    \"connector\": \"kafka\",\n",
        "    \"topic\": \"user_trans_amt_last_360min_windowed\",\n",
        "    \"properties.bootstrap.servers\": \"localhost:9092\",\n",
        "    \"format\": \"json\",\n",
        "}\n",
        "\n",
        "sink_table = connection.create_table(\n",
        "    \"user_trans_amt_last_360min_windowed\",\n",
        "    schema=sink_schema,\n",
        "    tbl_properties=sink_configs,\n",
        ")\n",
        "\n",
        "connection.insert(\n",
        "    \"user_trans_amt_last_360min_windowed\", user_trans_amt_last_360m_agg_windowed_stream\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected output\n",
        "\n",
        "Now, if everything is working correctly, you should expect to see results being\n",
        "streamed into the Kafka topic!\n",
        "\n",
        "::: {.callout-tip}\n",
        "You can inspect the Kafka topic using the Python Kafka client\n",
        "if you have it installed or via console Kafka consumer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    \"user_trans_amt_last_360min\"\n",
        ")  # or \"user_trans_amt_last_360min_windowed\"\n",
        "for _, msg in zip(range(10), consumer):\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Next steps\n",
        "\n",
        "Woohoo, great job! Now that you've connected to Flink and learned the basics, you\n",
        "can query your own data. See the rest of the Ibis documentation or [Flink\n",
        "documentation](https://nightlies.apache.org/flink/flink-docs-stable/). You can\n",
        "[open an issue](https://github.com/ibis-project/ibis/issues/new/choose) if you run\n",
        "into one!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
